{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-7a78502d5972>:20: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(['seaborn-white'])\n"
     ]
    }
   ],
   "source": [
    "import neurom as nm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from scipy.spatial import distance_matrix\n",
    "import gudhi as gd \n",
    "import pandas as pd\n",
    "import random, math, tmd, os, cv2, morphio\n",
    "from tmd.view import plot as tmdplt\n",
    "from tmd.Topology import analysis\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.stats import ttest_ind, f_oneway\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from Utilities import *\n",
    "plt.style.use(['seaborn-white'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### General variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "morpho_data_path = '../data/morphologies/swc30/'\n",
    "metadata_data_path = '../data/metadata/'\n",
    "thresholds_occur_regs = 30\n",
    "random_state = 1441"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loading of metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the CSV file containing brain regions into a pandas dataframe\n",
    "df = pd.read_csv(metadata_data_path + 'morph_regions.csv')\n",
    "morpho_labels_reg = df.iloc[:,0].to_numpy()\n",
    "regions_labels = df.iloc[:,1].to_numpy()\n",
    "regions_dict = {}\n",
    "for m in range(len(morpho_labels_reg)):\n",
    "    regions_dict[morpho_labels_reg[m]] = regions_labels[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file containing m_types regions into a pandas dataframe\n",
    "df = pd.read_csv(metadata_data_path + 'morpho_metadata_raw.csv')\n",
    "df = df[['neuron_name', 'cell_type']]\n",
    "morpho_names_tmp = df.iloc[:,0].to_numpy()\n",
    "morpho_names_m_types = np.asarray([morpho_name + '.swc' for morpho_name in morpho_names_tmp])\n",
    "m_types = df.iloc[:,1].to_numpy()\n",
    "\n",
    "# We select only those m_types appearing more than 10 times in the dataset\n",
    "unique_m_types, counts_m_types = np.unique(m_types, return_counts = True)\n",
    "m_types_OI = unique_m_types[counts_m_types > 10]\n",
    "\n",
    "# We look for the indices of the morphologies we are interested in keeping and we store them\n",
    "idx_morphos_OI = []\n",
    "for m_type in m_types_OI:\n",
    "    idx_morphos_OI.append(np.where(m_types == m_type)[0])\n",
    "idx_morphos_OI = np.concatenate(idx_morphos_OI)\n",
    "\n",
    "# We built a final dictionary mapping the name of a morphology to its m_type\n",
    "morpho_names_m_types_OI = morpho_names_m_types[idx_morphos_OI]\n",
    "m_types_OI_final = m_types[idx_morphos_OI]\n",
    "m_types_dict = {}\n",
    "for m in range(len(morpho_names_m_types_OI)):\n",
    "    m_types_dict[morpho_names_m_types_OI[m]] = m_types_OI_final[m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loading morphologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load the file.\n",
      "Could not load the file.\n",
      "Could not load the file.\n",
      "Could not load the file.\n",
      "1103 morphologies were loaded.\n"
     ]
    }
   ],
   "source": [
    "# We now load all the actual morphologies from the .swc files.\n",
    "# Not all files can be loaded and the regions and m_types are not necessarily in the same order as we load them but we use \n",
    "# the dictionaries we built to map them and store these info in the good order\n",
    "list_morpho = os.listdir(morpho_data_path)\n",
    "all_morphos = []\n",
    "all_names = []\n",
    "all_regs = []\n",
    "all_m_types = []\n",
    "for i, tmp_morpho in enumerate(m_types_dict.keys()):\n",
    "    if tmp_morpho.endswith('.swc'):\n",
    "        tmp_filename = morpho_data_path + tmp_morpho\n",
    "        try: \n",
    "            all_morphos.append(tmd.io.load_neuron_from_morphio(tmp_filename))\n",
    "            all_names.append(tmp_morpho)\n",
    "            all_regs.append(regions_dict[tmp_morpho])\n",
    "            all_m_types.append(m_types_dict[tmp_morpho])\n",
    "        except: print('Could not load the file.')\n",
    "print('%d morphologies were loaded.'%len(all_morphos))\n",
    "all_morphos = np.asarray(all_morphos)\n",
    "all_names = np.asarray(all_names)\n",
    "all_regs = np.asarray(all_regs)\n",
    "all_m_types = np.asarray(all_m_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the regions corresponding to the m_type 'granule' are changed to DG (after checking that it is indeed the case)\n",
    "# If we don't do this this m_type would be discarded since it does not appear in the same exact region more than the threshold \n",
    "# impose\n",
    "for m, m_type in enumerate(np.unique(all_m_types)):\n",
    "    if m_type.find('granule') != -1:\n",
    "        id_granule = m\n",
    "all_regs[all_m_types == np.unique(all_m_types)[id_granule]] = 'DG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the regions corresponding to the m_type 'Purkinje' are changed to Cerebellum (after checking that it is indeed the case)\n",
    "# If we don't do this this m_type would be discarded since it does not appear in the same exact region more than the threshold \n",
    "# impose\n",
    "for m, m_type in enumerate(np.unique(all_m_types)):\n",
    "    if m_type.find('Purkinje') != -1:\n",
    "        id_purkinje = m\n",
    "all_regs[all_m_types == np.unique(all_m_types)[id_purkinje]] = 'Cerebellum'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Getting only the axon neurite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each morphology loaded, we select only the axonal part since the focus of our analysis\n",
    "all_morpho_axons = [morpho.axon[0] for morpho in all_morphos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Selecting only the samples (morphologies) belonging to regions with a certain number of occurencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 regions passing the threshold over the minimal number of occurencies.\n",
      "There regions are : ['Cerebellum' 'DG' 'MOs2/3' 'MOs5' 'MOs6a' 'PRE' 'SUB' 'VAL']\n"
     ]
    }
   ],
   "source": [
    "unique_regions, counts_regions = np.unique(all_regs, return_counts = True)\n",
    "regions_desired = unique_regions[counts_regions > thresholds_occur_regs]\n",
    "print('There are %d regions passing the threshold over the minimal number of occurencies.'%len(regions_desired))\n",
    "print('There regions are :', regions_desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We store the indices of the morphologies belonging to the regions of interest identified \n",
    "idx_regions_desired = {}\n",
    "for reg in regions_desired:\n",
    "    idx_regions_desired[reg] = np.where(np.asarray(all_regs) == reg)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of morphos :  512\n",
      "Number of file names :  512\n",
      "Number of m_types :  512\n",
      "Number of regs :  512\n"
     ]
    }
   ],
   "source": [
    "# We rebuild the variables that we want by selecting now only the morphologies belong to the regions OI\n",
    "morphos_OI = []\n",
    "regs_OI = []\n",
    "names_OI = []\n",
    "m_types_OI = []\n",
    "for reg in idx_regions_desired.keys():\n",
    "    morphos_OI.append(np.asarray(all_morpho_axons)[idx_regions_desired[reg]])\n",
    "    regs_OI.append([reg] * len(idx_regions_desired[reg]))\n",
    "    names_OI.append(np.asarray(all_names)[idx_regions_desired[reg]])\n",
    "    m_types_OI.append(np.asarray(all_m_types)[idx_regions_desired[reg]])\n",
    "morphos_OI = np.asarray(np.concatenate(morphos_OI))\n",
    "regs_OI = np.asarray(np.concatenate(regs_OI))\n",
    "names_OI = np.asarray(np.concatenate(names_OI))\n",
    "m_types_OI = np.asarray(np.concatenate(m_types_OI))\n",
    "n_morphos = len(morphos_OI)\n",
    "print('Number of morphos : ', n_morphos)\n",
    "print('Number of file names : ', len(names_OI))\n",
    "print('Number of m_types : ', len(m_types_OI))\n",
    "print('Number of regs : ', len(regs_OI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computing the persistent diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute now the persistent diagrams of the axonal tress of the selected morphologies.\n",
    "# We do so by computing it both by using the 'radial distance' and the 'path distance' as filtration metrics\n",
    "all_pd = {'rad' : [], 'path' : []}\n",
    "for axon_tree in morphos_OI:\n",
    "    all_pd['rad'].append(tmd.methods.get_persistence_diagram(axon_tree, feature = \"radial_distances\"))\n",
    "    all_pd['path'].append(tmd.methods.get_persistence_diagram(axon_tree, feature = \"path_distances\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computing the persistent images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonardopollina/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/kde.py:563: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  self._data_covariance = atleast_2d(cov(self.dataset, rowvar=1,\n",
      "/Users/leonardopollina/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/function_base.py:2680: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "/Users/leonardopollina/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/function_base.py:2680: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persistent image has not been computed.\n",
      "Persistent image has not been computed.\n",
      "Persistent image has not been computed.\n",
      "Persistent image has not been computed.\n",
      "Now there are 510 neurons.\n",
      "510\n",
      "510\n",
      "510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonardopollina/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/function_base.py:5030: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n"
     ]
    }
   ],
   "source": [
    "# We compute the persistent images by using the persistent diagrams we already computed. \n",
    "# - We are scaling all the persistent images to the same scales (xlims, ylims)\n",
    "# - We are using as norm_factor 1 in order to NOT normalize for now the persistent images\n",
    "all_pi = {'rad': [], 'path' : []}\n",
    "not_computed_idx = []\n",
    "for metric in all_pd.keys():\n",
    "    # Computing the xlim and ylim in order to scale all the persistent images accordingly\n",
    "    xlims, ylims = analysis.get_limits(all_pd[metric])\n",
    "    for i, tmp_pd in enumerate(all_pd[metric]):\n",
    "        try: \n",
    "            # We set norm factor to 1 because we actually don't want to normalize the persistent images individually\n",
    "            all_pi[metric].append(analysis.get_persistence_image_data(tmp_pd, xlim = xlims, ylim = ylims, norm_factor = 1))\n",
    "        except: \n",
    "            print('Persistent image has not been computed.')\n",
    "            not_computed_idx.append(i)\n",
    "all_pd_OI = all_pd\n",
    "all_pd_OI['rad'] = np.delete(all_pd['rad'], np.unique(not_computed_idx))\n",
    "all_pd_OI['path'] = np.delete(all_pd['path'], np.unique(not_computed_idx))\n",
    "names_OI = np.delete(names_OI, np.unique(not_computed_idx))\n",
    "m_types_OI = np.delete(m_types_OI, np.unique(not_computed_idx))\n",
    "regs_OI = np.delete(regs_OI, np.unique(not_computed_idx))\n",
    "n_morphos = len(names_OI)\n",
    "print('Now there are %d neurons.'%np.shape(all_pi[metric])[0])\n",
    "print(len(names_OI))\n",
    "print(len(m_types_OI))\n",
    "print(len(regs_OI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To normalize we divide each persistent images by the sum of its values in the picture (this makes each image a sort of \n",
    "# probabilistic map). Then we multiply them per the number of branches there were in the diagram (in order to take into account \n",
    "# the differences in the number of branches).\n",
    "all_pi_norm = {}\n",
    "for metric in all_pi.keys():\n",
    "    all_pi_norm[metric] = []\n",
    "    for im in range(len(all_pi[metric])):\n",
    "        n_branches = len(all_pd_OI[metric][im])\n",
    "        all_pi_norm[metric].append(n_branches * all_pi[metric][im]/np.sum(all_pi[metric][im]))\n",
    "    all_pi_norm[metric] = np.asarray(all_pi_norm[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_variable(all_pi_norm, '../Results/Saved_variables/all_persistent_images_normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['principal cell', 'pyramidal', 'projection']\n",
      "MOs6a\n"
     ]
    }
   ],
   "source": [
    "a = np.where(names_OI == 'AA0012.swc')[0][0]\n",
    "print(m_types_OI[a])\n",
    "print(regs_OI[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['principal cell', 'pyramidal', 'projection']\n",
      "MOs5\n"
     ]
    }
   ],
   "source": [
    "b = np.where(names_OI == 'AA0130.swc')[0][0]\n",
    "print(m_types_OI[b])\n",
    "print(regs_OI[b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plotting Persistent Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just to plot the persistent images all on the same scale and also normalized as explained above\n",
    "for metric in all_pd.keys():\n",
    "    # Computing the xlim and ylim in order to scale all the persistent images accordingly\n",
    "    xlims, ylims = analysis.get_limits(all_pd_OI[metric])\n",
    "    for im, tmp_pd in enumerate(all_pd_OI[metric]):\n",
    "        n_branches = len(tmp_pd)\n",
    "        norm_fact = np.sum(all_pi[metric][im])/n_branches\n",
    "        vmax = np.max(all_pi[metric][im]/norm_fact)\n",
    "        try: \n",
    "            if metric == 'rad': tmp , _ = tmdplt.persistence_image(tmp_pd, xlim = xlims, ylim = ylims, vmin = 0, vmax = vmax, norm_factor = norm_fact)\n",
    "            else: tmp , _ = tmdplt.persistence_image(tmp_pd, xlim = xlims, ylim = ylims, vmin = 0, vmax = vmax, norm_factor = norm_fact, xlabel = 'Birth (distant from soma)', ylabel = 'Death (distant from soma)')\n",
    "            plt.savefig('../Results/Persistent_images/New_scaled_images/%s/%s.png'%(metric, names_OI[im].split('.')[0]), bbox_inches = 'tight')\n",
    "        except: \n",
    "            print('Persistent image has not been computed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Merging the subregions of MO and make them all 'MO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_regs = []\n",
    "for reg in regs_OI:\n",
    "    if reg[:2] == 'MO': merged_regs.append('MO')\n",
    "    else: merged_regs.append(reg)\n",
    "merged_regs = np.asarray(merged_regs)\n",
    "print('The new (merged) regions are:')\n",
    "for i in np.unique(merged_regs):\n",
    "    print('- %s'%i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Restructuring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We flatten the persistent images in order to have a final matrix having shape N x F, where N = number of samples (neurons) and\n",
    "# F = number of features\n",
    "flatten_pi = {}\n",
    "for metric in all_pi_norm.keys():\n",
    "    tmp_data = all_pi_norm[metric]\n",
    "    n_samples = np.shape(tmp_data)[0]\n",
    "    flatten_pi[metric] = np.reshape(tmp_data, (n_samples, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Applying dimensionality reduction (tSNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embedded = {}\n",
    "for metric in flatten_pi.keys():\n",
    "    tmp_data = flatten_pi[metric]\n",
    "    data_embedded[metric] = TSNE(n_components = 2, perplexity = np.round(np.sqrt(n_morphos)), random_state = random_state).fit_transform(tmp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_variable(data_embedded, '../Results/Saved_variables/data_embedded_TSNE_n2_random_state%d'%random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We build a labels vector for different types of labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original regions of interested (not merged MO layers)\n",
    "labels_regs = np.unique(regs_OI)\n",
    "idx_labels_regs = {}\n",
    "for label in labels_regs:\n",
    "    idx_labels_regs[label] = np.where(regs_OI == label)[0]\n",
    "\n",
    "# Merged regions labels\n",
    "labels_merged_regs = np.unique(merged_regs)\n",
    "idx_labels_merged_regs = {}\n",
    "for label in labels_merged_regs:\n",
    "    idx_labels_merged_regs[label] = np.where(merged_regs == label)[0]\n",
    "\n",
    "# M-types layers\n",
    "labels_m_types = np.unique(m_types_OI)\n",
    "idx_labels_m_types = {}\n",
    "for label in labels_m_types:\n",
    "    idx_labels_m_types[label] = np.where(m_types_OI == label)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plotting t-SNE visualization following different labels or metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "curr_labels = idx_labels_regs\n",
    "plt.figure(figsize = (20, 8))\n",
    "sns.set_palette(\"pastel\")\n",
    "for m, metric in enumerate(data_embedded.keys()):\n",
    "    tmp_embedded_data = data_embedded[metric]\n",
    "    all_data = []\n",
    "    plt.subplot(1,2,m+1)\n",
    "    for lab in curr_labels.keys():\n",
    "        x = tmp_embedded_data[curr_labels[lab],0]\n",
    "        y = tmp_embedded_data[curr_labels[lab],1]\n",
    "        if m == 1: sns.scatterplot(x,y, label = lab, s = 100)\n",
    "        else: sns.scatterplot(x,y,s = 100)\n",
    "    if m == 1: plt.legend(fontsize = 14, bbox_to_anchor = (1.15,1))\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.xlabel('1st Component', fontsize = 18, weight = 'bold')\n",
    "    plt.ylabel('2nd Component', fontsize = 18, weight = 'bold')\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.title(metric, weight = 'bold', fontsize = 24)\n",
    "plt.savefig('../Results/Figures/New_TSNE_proj_n2_regs.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_labels = idx_labels_merged_regs\n",
    "plt.figure(figsize = (20, 8))\n",
    "sns.set_palette(\"pastel\")\n",
    "for m, metric in enumerate(data_embedded.keys()):\n",
    "    tmp_embedded_data = data_embedded[metric]\n",
    "    all_data = []\n",
    "    plt.subplot(1,2,m+1)\n",
    "    for lab in curr_labels.keys():\n",
    "        x = tmp_embedded_data[curr_labels[lab],0]\n",
    "        y = tmp_embedded_data[curr_labels[lab],1]\n",
    "        if m == 1: sns.scatterplot(x,y, label = lab, s = 100)\n",
    "        else: sns.scatterplot(x,y,s = 100)\n",
    "    if m == 1: plt.legend(fontsize = 14, bbox_to_anchor = (1.15,1))\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.xlabel('1st Component', fontsize = 18, weight = 'bold')\n",
    "    plt.ylabel('2nd Component', fontsize = 18, weight = 'bold')\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.title(metric, weight = 'bold', fontsize = 24)\n",
    "plt.savefig('../Results/Figures/New_TSNE_proj_n2_merged_regs.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_labels = idx_labels_m_types \n",
    "plt.figure(figsize = (20, 8))\n",
    "sns.set_palette(\"pastel\")\n",
    "for m, metric in enumerate(data_embedded.keys()):\n",
    "    tmp_embedded_data = data_embedded[metric]\n",
    "    all_data = []\n",
    "    plt.subplot(1,2,m+1)\n",
    "    for lab in curr_labels.keys():\n",
    "        x = tmp_embedded_data[curr_labels[lab],0]\n",
    "        y = tmp_embedded_data[curr_labels[lab],1]\n",
    "        if m == 1: sns.scatterplot(x,y, label = lab, s = 100)\n",
    "        else: sns.scatterplot(x,y,s = 100)\n",
    "    if m == 1: plt.legend(fontsize = 14, bbox_to_anchor = (1.05,1))\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.xlabel('1st Component', fontsize = 18, weight = 'bold')\n",
    "    plt.ylabel('2nd Component', fontsize = 18, weight = 'bold')\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.title(metric, weight = 'bold', fontsize = 24)\n",
    "plt.savefig('../Results/Figures/New_TSNE_proj_n2_mtypes.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now selecting only the neurons belonging to the m_type pyramidal or projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select the two classes of m_types that we see overlap a lot in the projection using t-SNE \n",
    "mt1 = \"['principal cell', 'projection']\"\n",
    "mt2 = \"['principal cell', 'pyramidal', 'projection']\"\n",
    "\n",
    "idx1 = np.where(m_types_OI == mt1)[0]\n",
    "idx2 = np.where(m_types_OI == mt2)[0]\n",
    "all_idx_OI = np.concatenate([idx1, idx2])\n",
    "sel_names = names_OI[all_idx_OI]\n",
    "n_sel_morphos = len(all_idx_OI)\n",
    "print('There are %s selected morphos.'%n_sel_morphos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embedded_sel = {}\n",
    "for metric in data_embedded.keys():\n",
    "    data_embedded_sel[metric] = data_embedded[metric][all_idx_OI, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_variable(data_embedded_sel, '../Results/Saved_variables/data_embedded_selected_morphos_TSNE_n2_random_state%d'%random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_labels = idx_labels_m_types \n",
    "plt.figure(figsize = (20, 8))\n",
    "sns.set_palette(\"Blues\")\n",
    "for m, metric in enumerate(data_embedded_sel.keys()):\n",
    "    tmp_embedded_data = data_embedded_sel[metric]\n",
    "    all_data = []\n",
    "    plt.subplot(1,2,m+1)\n",
    "    x = tmp_embedded_data[:,0]\n",
    "    y = tmp_embedded_data[:,1]\n",
    "    sns.scatterplot(x,y,s = 100)\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.xlabel('1st Component', fontsize = 18, weight = 'bold')\n",
    "    plt.ylabel('2nd Component', fontsize = 18, weight = 'bold')\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.title(metric, weight = 'bold', fontsize = 24)\n",
    "plt.savefig('../Results/Figures/New_TSNE_proj_n2_selected_morphos.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Trying to cluster the projected morphos using  K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We try to optimize the number of clusters using the elbow method \n",
    "# Also, because of randomness present in K-means initialization, we repeat the procedure 10 times and we plot only the average of\n",
    "# the sse values  (Sum of Squared Errors)\n",
    "k_values = range(1, 11)  # Possible number of clusters\n",
    "all_sse = {}\n",
    "for metric in data_embedded_sel.keys():\n",
    "    all_sse[metric] = []\n",
    "    for rep in range(20):\n",
    "        tmp_data = data_embedded_sel[metric]\n",
    "        sse = []\n",
    "        for k in k_values:\n",
    "            kmeans = KMeans(n_clusters = k, n_init = 'auto')\n",
    "            kmeans.fit(tmp_data)\n",
    "            sse.append(kmeans.inertia_)\n",
    "        all_sse[metric].append(sse)\n",
    "    all_sse[metric] = np.asarray(all_sse[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing mean and std over the 20 repetitions\n",
    "sse_mean_rad = np.mean(all_sse['rad'], axis = 0)\n",
    "sse_std_rad = np.std(all_sse['rad'], axis = 0)\n",
    "sse_mean_path = np.mean(all_sse['path'], axis = 0)\n",
    "sse_std_path = np.std(all_sse['path'], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,5))\n",
    "# RADIAL\n",
    "plt.plot(k_values, sse_mean_rad, ls = '-', lw = 4, marker = '.', ms = 20, label = 'Radial distance')\n",
    "plt.plot(k_values, sse_mean_path, ls = '-', lw = 4, marker = '.', ms = 20, label = 'Path distance')\n",
    "plt.xlabel('Number of Clusters (K)', fontsize = 14, weight = 'bold')\n",
    "plt.ylabel('Sum of Squared Distances', fontsize = 14, weight = 'bold')\n",
    "plt.title('Elbow method', fontsize = 18, weight = 'bold')\n",
    "plt.xticks(k_values, fontsize = 14, weight = 'bold')\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.legend(fontsize = 18, bbox_to_anchor = (1.05,1))\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.savefig('../Results/Figures/New_Elbow_method_opt_kmeans.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "sns.set_palette(\"Blues\")\n",
    "metric = 'path'\n",
    "plt.figure(figsize = (8, 8))\n",
    "kmeans = KMeans(n_clusters = k, n_init = 'auto', random_state = random_state).fit(data_embedded_sel[metric])\n",
    "labels = kmeans.labels_\n",
    "for l in np.unique(labels):\n",
    "    x = data_embedded_sel[metric][labels == l,0]\n",
    "    y = data_embedded_sel[metric][labels == l,1]\n",
    "    sns.scatterplot(x,y, s = 100, label = 'Cluster %d'%l)\n",
    "    plt.legend(fontsize = 18, bbox_to_anchor = (1.2,1))\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.xlabel('1st Component', fontsize = 18, weight = 'bold')\n",
    "    plt.ylabel('2nd Component', fontsize = 18, weight = 'bold')\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.title(metric, weight = 'bold', fontsize = 24)\n",
    "plt.savefig('../Results/Figures/New_Cluster_kmeans_%s.png'%metric, dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Trying the clustering via DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'rad'\n",
    "eps_values = np.linspace(0.1,5,100)\n",
    "min_samples = np.arange(5,101,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to find the identify the best hyperparameters by maximizing the Silhouette Score\n",
    "all_scores = np.zeros((len(eps_values), len(min_samples)))\n",
    "all_n_clusters = np.zeros((len(eps_values), len(min_samples)))\n",
    "for e, eps in enumerate(eps_values): \n",
    "    for s, min_sample in enumerate(min_samples):\n",
    "        dbscan = DBSCAN(eps = eps, min_samples = min_sample)\n",
    "        dbscan.fit(data_embedded_sel[metric])\n",
    "        labels = dbscan.labels_\n",
    "        # Print the number of clusters (excluding noise points)\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        all_n_clusters[e,s] = n_clusters\n",
    "        # Evaluate the clustering results\n",
    "        if n_clusters > 1:\n",
    "            silhouette_score = metrics.silhouette_score(data_embedded_sel[metric], labels)\n",
    "            all_scores[e,s] = silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eps_id, best_min_sample_id = np.where(all_scores == np.max(all_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eps_id, best_min_sample_id = np.where(all_scores == np.max(all_scores))\n",
    "if len(best_eps_id) > 1: best_eps_id = best_eps_id[0]\n",
    "if len(best_min_sample_id) > 1: best_min_sample_id = best_min_sample_id[0]\n",
    "print('The best parameters found are: eps = %0.2f, min. number of samples = %d'%(eps_values[best_eps_id], min_samples[best_min_sample_id]))\n",
    "print('The number of clusters found for these values is %d.'%all_n_clusters[best_eps_id, best_min_sample_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('Blues')\n",
    "plt.figure(figsize = (6,5))\n",
    "imshow_obj = plt.imshow(all_scores, aspect = 'auto', cmap = 'Blues', alpha = 0.8)\n",
    "ax = plt.gca()\n",
    "# X axis ticks\n",
    "xtick_labels = min_samples[[0, 4, 9, 14, 19]]\n",
    "ax.set_xticks([0, 4, 9, 14, 19]) \n",
    "ax.set_xticklabels(xtick_labels)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.xlabel('Min. number of samples', fontsize = 18, weight = 'bold')\n",
    "# Y axis ticks\n",
    "ytick_labels = np.round(eps_values[[0, 19, 39, 59, 79, 99]],1)\n",
    "ax.set_yticks([0, 19, 39, 59, 79, 99]) \n",
    "ax.set_yticklabels(ytick_labels)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.ylabel('Epsilon', fontsize = 18, weight = 'bold')\n",
    "cbar = plt.colorbar(imshow_obj, ax = ax, aspect = 20, pad = 0.02)\n",
    "cbar.set_label('Silhouette score', fontsize = 18, weight = 'bold') \n",
    "cbar.ax.tick_params(labelsize = 14) \n",
    "sns.set_style(\"white\")\n",
    "plt.savefig('../Results/Figures/Silhouette_score_opt_DBSCAN_%s.png'%metric, dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 8))\n",
    "sns.set_palette(\"pastel\")\n",
    "dbscan = DBSCAN(eps = eps_values[best_eps_id], min_samples = min_samples[best_min_sample_id])\n",
    "dbscan.fit(data_embedded_sel[metric])\n",
    "labels = dbscan.labels_\n",
    "for l in np.unique(labels):\n",
    "    x = data_embedded_sel[metric][labels == l,0]\n",
    "    y = data_embedded_sel[metric][labels == l,1]\n",
    "    if l == -1: sns.scatterplot(x,y, s = 100, label = 'Noise')\n",
    "    else: sns.scatterplot(x,y, s = 100, label = 'Cluster %d'%(l+1))\n",
    "    plt.legend(fontsize = 18, bbox_to_anchor = (1.2,1))\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.xlabel('1st Component', fontsize = 18, weight = 'bold')\n",
    "    plt.ylabel('2nd Component', fontsize = 18, weight = 'bold')\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.title(metric, weight = 'bold', fontsize = 24)\n",
    "plt.savefig('../Results/Figures/New_Cluster_DBSCAN_%s.png'%metric, dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Trying the clustering via Gaussian-Mixture Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We try to optimize the number of clusters for GMM using the Silhouette Score\n",
    "k_values = range(2, 11)  # Possible number of clusters\n",
    "silhouette_scores = {}\n",
    "for metric in data_embedded_sel.keys():\n",
    "    silhouette_scores[metric] = []\n",
    "    for rep in range(20):\n",
    "        tmp_data = data_embedded_sel[metric]\n",
    "        scores = []\n",
    "        for k in k_values:\n",
    "            gmm = GaussianMixture(n_components = k)\n",
    "            labels = gmm.fit_predict(tmp_data)\n",
    "            scores.append(metrics.silhouette_score(tmp_data, labels))\n",
    "        silhouette_scores[metric].append(scores)\n",
    "    silhouette_scores[metric] = np.asarray(silhouette_scores[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('Blues')\n",
    "plt.figure(figsize = (9,5))\n",
    "for metric in silhouette_scores.keys():\n",
    "    plt.plot(k_values, np.mean(silhouette_scores[metric], axis = 0), lw = 4, label = metric)\n",
    "plt.legend(fontsize = 18)\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.xticks(fontsize = 18, weight = 'bold')\n",
    "plt.yticks(fontsize = 18)\n",
    "plt.xlabel('N. clusters', fontsize = 18, weight = 'bold')\n",
    "plt.ylabel('Silhouette Score', fontsize = 18, weight = 'bold')\n",
    "plt.savefig('../Results/Figures/New_Silhouette_GMMs.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(silhouette_scores['rad'], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_rad = k_values[np.argmax(np.mean(silhouette_scores['rad'], axis = 0))]\n",
    "k_path = k_values[np.argmax(np.mean(silhouette_scores['path'], axis = 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'path'\n",
    "if metric == 'rad': k = k_rad\n",
    "if metric == 'path': k = k_path\n",
    "plt.figure(figsize = (8, 8))\n",
    "sns.set_palette(\"pastel\")\n",
    "gmm = GaussianMixture(n_components = k, random_state = random_state)\n",
    "labels = gmm.fit_predict(data_embedded_sel[metric])\n",
    "for l in np.unique(labels):\n",
    "    x = data_embedded_sel[metric][labels == l,0]\n",
    "    y = data_embedded_sel[metric][labels == l,1]\n",
    "    sns.scatterplot(x,y, s = 100, label = 'Cluster %d'%(l+1))\n",
    "    plt.legend(fontsize = 18, bbox_to_anchor = (1.2,1))\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.xlabel('1st Component', fontsize = 18, weight = 'bold')\n",
    "    plt.ylabel('2nd Component', fontsize = 18, weight = 'bold')\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.title(metric, weight = 'bold', fontsize = 24)\n",
    "plt.savefig('../Results/Figures/New_Cluster_GMMs_%s.png'%metric, dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computing how much the clusters found using Path Distance and Radial Distance overlap (GMMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_rad = 2\n",
    "k_path = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_rad = GaussianMixture(n_components = k_rad, random_state = random_state)\n",
    "labels_gmm_rad = gmm_rad.fit_predict(data_embedded_sel['rad'])\n",
    "gmm_path = GaussianMixture(n_components = k_path, random_state = random_state)\n",
    "labels_gmm_path = gmm_path.fit_predict(data_embedded_sel['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_rad = {}\n",
    "for lab in np.unique(labels_gmm_rad):\n",
    "    sets_rad[lab] = set(np.where(labels_gmm_rad == lab)[0])\n",
    "sets_path = {}\n",
    "for lab in np.unique(labels_gmm_path):\n",
    "    sets_path[lab] = set(np.where(labels_gmm_path == lab)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_intersec = []\n",
    "for set1 in sets_rad.keys():\n",
    "    for set2 in sets_path.keys():\n",
    "        inter = sets_rad[set1].intersection(sets_path[set2])\n",
    "        vals_intersec.append(100* len(inter)/len(sets_rad[set1]))\n",
    "vals_intersec = np.asarray(vals_intersec)\n",
    "vals_intersec = np.reshape(vals_intersec, (k_rad,k_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,4))\n",
    "plt.imshow(vals_intersec, aspect = 'auto', cmap = 'Blues', vmin = 0, vmax = 100)\n",
    "\n",
    "# Labels\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(k_path))\n",
    "ax.set_yticks(np.arange(k_rad))\n",
    "ax.set_xticklabels(range(1, k_path + 1), fontsize = 18, weight = 'bold')\n",
    "ax.set_yticklabels(range(1, k_rad + 1), fontsize = 18, weight = 'bold')\n",
    "plt.setp(ax.get_xticklabels(), ha = 'right', rotation_mode = 'anchor')\n",
    "plt.ylabel('Radial', fontsize = 24, weight = 'bold')\n",
    "plt.xlabel('Path', fontsize = 24, weight = 'bold')\n",
    "\n",
    "# Loop over data dimensions and create text annotations to show the percentage of percentage of predictions for each combination\n",
    "for i in range(k_rad):\n",
    "    for j in range(k_path):\n",
    "        if vals_intersec[i, j] > 50:\n",
    "            text = ax.text(j, i, str(round(vals_intersec[i, j],2)) + '%', ha = 'center', va = 'center', color = 'white', fontsize = 18)\n",
    "        if vals_intersec[i, j] < 50:\n",
    "            text = ax.text(j, i, str(round(vals_intersec[i, j],2)) + '%', ha = 'center', va = 'center', color = 'black', fontsize = 18)\n",
    "\n",
    "plt.title('Intersection in \\nidentified clusters', fontsize = 28, weight = 'bold')         \n",
    "plt.savefig('../Results/Figures/New_Perc_intersec_clusters_GMMs.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### &rarr; Computing how much the clusters found using Path Distance and Radial Distance overlap (DBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_path = DBSCAN(eps = 4.65, min_samples = 15)\n",
    "dbscan_path.fit(data_embedded_sel['path'])\n",
    "labels_DBSCAN_path = dbscan_path.labels_\n",
    "\n",
    "dbscan_rad = DBSCAN(eps = 4.11, min_samples = 10)\n",
    "dbscan_rad.fit(data_embedded_sel['rad'])\n",
    "labels_DBSCAN_rad = dbscan_rad.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_DBSCAN_path = len(np.unique(labels_DBSCAN_path))\n",
    "n_clusters_DBSCAN_rad = len(np.unique(labels_DBSCAN_rad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_DBSCAN_rad = {}\n",
    "for lab in np.unique(labels_DBSCAN_rad):\n",
    "    sets_DBSCAN_rad[lab] = set(np.where(labels_DBSCAN_rad == lab)[0])\n",
    "sets_DBSCAN_path = {}\n",
    "for lab in np.unique(labels_DBSCAN_path):\n",
    "    sets_DBSCAN_path[lab] = set(np.where(labels_DBSCAN_path == lab)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_intersec_DBSCAN = []\n",
    "for set1 in sets_DBSCAN_rad.keys():\n",
    "    for set2 in sets_DBSCAN_path.keys():\n",
    "        inter = sets_DBSCAN_rad[set1].intersection(sets_DBSCAN_path[set2])\n",
    "        vals_intersec_DBSCAN.append(100* len(inter)/len(sets_DBSCAN_rad[set1]))\n",
    "vals_intersec_DBSCAN = np.asarray(vals_intersec_DBSCAN)\n",
    "vals_intersec_DBSCAN = np.reshape(vals_intersec_DBSCAN, (n_clusters_DBSCAN_rad, n_clusters_DBSCAN_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.imshow(vals_intersec_DBSCAN, aspect = 'auto', cmap = 'Blues', vmin = 0, vmax = 100)\n",
    "\n",
    "# Labels\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(n_clusters_DBSCAN_path))\n",
    "ax.set_yticks(np.arange(n_clusters_DBSCAN_rad))\n",
    "ax.set_xticklabels(['Noise', '1', '2', '3', '4', '5'], fontsize = 18, weight = 'bold')\n",
    "ax.set_yticklabels(['Noise', '1', '2', '3'], fontsize = 18, weight = 'bold')\n",
    "plt.setp(ax.get_xticklabels(), ha = 'right', rotation_mode = 'anchor')\n",
    "plt.ylabel('Radial', fontsize = 24, weight = 'bold')\n",
    "plt.xlabel('Path', fontsize = 24, weight = 'bold')\n",
    "\n",
    "# Loop over data dimensions and create text annotations to show the percentage of percentage of predictions for each combination\n",
    "for i in range(n_clusters_DBSCAN_rad):\n",
    "    for j in range(n_clusters_DBSCAN_path):\n",
    "        if vals_intersec_DBSCAN[i, j] > 50:\n",
    "            text = ax.text(j, i, str(round(vals_intersec_DBSCAN[i, j],2)) + '%', ha = 'center', va = 'center', color = 'white', fontsize = 18)\n",
    "        if vals_intersec_DBSCAN[i, j] < 50:\n",
    "            text = ax.text(j, i, str(round(vals_intersec_DBSCAN[i, j],2)) + '%', ha = 'center', va = 'center', color = 'black', fontsize = 18)\n",
    "\n",
    "plt.title('Intersection in \\nidentified clusters', fontsize = 28, weight = 'bold')         \n",
    "plt.savefig('../Results/Figures/New_Perc_intersec_clusters_DBSCAN.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Saving in pickle dictionaries the names of the morphos that belong to the different clusters\n",
    "- per metric (radial and path distances)\n",
    "- per clustering method (Kmeans and GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM\n",
    "names_gmm_rad = {}\n",
    "for l in np.unique(labels_gmm_rad):\n",
    "    names_gmm_rad[l] = sel_names[labels_gmm_rad == l]\n",
    "save_variable(names_gmm_rad, '../Results/Saved_variables/New_Cluster_dict_names_morphos_gmms_rad')\n",
    "\n",
    "names_gmm_path = {}\n",
    "for l in np.unique(labels_gmm_path):\n",
    "    names_gmm_path[l] = sel_names[labels_gmm_path == l]\n",
    "save_variable(names_gmm_path, '../Results/Saved_variables/New_Cluster_dict_names_morphos_gmms_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "\n",
    "#[1:] is to remove the \"noise cluster\"\n",
    "names_DBSCAN_rad = {}\n",
    "for l in np.unique(labels_DBSCAN_rad)[1:]:\n",
    "    names_DBSCAN_rad[l] = sel_names[labels_DBSCAN_rad == l]\n",
    "save_variable(names_DBSCAN_rad, '../Results/Saved_variables/New_Cluster_dict_names_morphos_DBSCAN_rad')\n",
    "\n",
    "names_DSCAN_path = {}\n",
    "for l in np.unique(labels_DBSCAN_path)[1:]:\n",
    "    names_DSCAN_path[l] = sel_names[labels_DBSCAN_path == l]\n",
    "save_variable(names_DSCAN_path, '../Results/Saved_variables/New_Cluster_dict_names_morphos_DBSCAN_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loading the cluster assignments and the morphometrics computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusters_GMM_rad = load_pickle_variable('../Results/Saved_variables/New_Cluster_dict_names_morphos_gmms_rad')\n",
    "clusters_GMM_path = load_pickle_variable('../Results/Saved_variables/New_Cluster_dict_names_morphos_gmms_path')\n",
    "clusters_DBSCAN_rad = load_pickle_variable('../Results/Saved_variables/New_Cluster_dict_names_morphos_DBSCAN_rad')\n",
    "clusters_DBSCAN_path = load_pickle_variable('../Results/Saved_variables/New_Cluster_dict_names_morphos_DBSCAN_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morphometrics_file_name = '../data/morphologies/morphometrics/21_features_morph_names_axon_means.csv'\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df_morphometrics = pd.read_csv(morphometrics_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morphometrics_neuron_names = df_morphometrics.morph_name.to_numpy()\n",
    "morphometrics_feat_names = df_morphometrics.feature_name.to_numpy()\n",
    "morphometrics_feat_vals = df_morphometrics.feature_val.to_numpy()\n",
    "# morphometrics_locations = df_morphometrics.iloc[:,4].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Separate the features (morphometrics) into the clusters identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM RADIAL DISTANCE\n",
    "all_features_GMM_rad = {}\n",
    "for cluster in clusters_GMM_rad.keys():\n",
    "    curr_cluster_morphos = clusters_GMM_rad[cluster]\n",
    "    all_features_GMM_rad[cluster] = {'Names' : [], 'Vals' : []}\n",
    "    for morpho_name in curr_cluster_morphos:\n",
    "        curr_morpho_flags = morphometrics_neuron_names == morpho_name\n",
    "        if np.sum(curr_morpho_flags) > 0:\n",
    "            all_features_GMM_rad[cluster]['Vals'].append(morphometrics_feat_vals[curr_morpho_flags])\n",
    "            all_features_GMM_rad[cluster]['Names'].append(morphometrics_feat_names[curr_morpho_flags])\n",
    "        else:\n",
    "            print('Neuron not found in the morphometrics file!')\n",
    "    all_features_GMM_rad[cluster]['Vals'] = np.asarray(all_features_GMM_rad[cluster]['Vals'])\n",
    "    all_features_GMM_rad[cluster]['Names'] = np.asarray(all_features_GMM_rad[cluster]['Names'])\n",
    "    \n",
    "# GMM PATH DISTANCE\n",
    "all_features_GMM_path = {}\n",
    "for cluster in clusters_GMM_path.keys():\n",
    "    curr_cluster_morphos = clusters_GMM_path[cluster]\n",
    "    all_features_GMM_path[cluster] = {'Names' : [], 'Vals' : []}\n",
    "    for morpho_name in curr_cluster_morphos:\n",
    "        curr_morpho_flags = morphometrics_neuron_names == morpho_name\n",
    "        if np.sum(curr_morpho_flags) > 0:\n",
    "            all_features_GMM_path[cluster]['Vals'].append(morphometrics_feat_vals[curr_morpho_flags])\n",
    "            all_features_GMM_path[cluster]['Names'].append(morphometrics_feat_names[curr_morpho_flags])\n",
    "        else:\n",
    "            print('Neuron not found in the morphometrics file!')\n",
    "    all_features_GMM_path[cluster]['Vals'] = np.asarray(all_features_GMM_path[cluster]['Vals'])\n",
    "    all_features_GMM_path[cluster]['Names'] = np.asarray(all_features_GMM_path[cluster]['Names'])\n",
    "    \n",
    "# DBSCAN RADIAL DISTANCE\n",
    "all_features_DBSCAN_rad = {}\n",
    "for cluster in clusters_DBSCAN_rad.keys():\n",
    "    curr_cluster_morphos = clusters_DBSCAN_rad[cluster]\n",
    "    all_features_DBSCAN_rad[cluster] = {'Names' : [], 'Vals' : []}\n",
    "    for morpho_name in curr_cluster_morphos:\n",
    "        curr_morpho_flags = morphometrics_neuron_names == morpho_name\n",
    "        if np.sum(curr_morpho_flags) > 0:\n",
    "            all_features_DBSCAN_rad[cluster]['Vals'].append(morphometrics_feat_vals[curr_morpho_flags])\n",
    "            all_features_DBSCAN_rad[cluster]['Names'].append(morphometrics_feat_names[curr_morpho_flags])\n",
    "        else:\n",
    "            print('Neuron not found in the morphometrics file!')\n",
    "    all_features_DBSCAN_rad[cluster]['Vals'] = np.asarray(all_features_DBSCAN_rad[cluster]['Vals'])\n",
    "    all_features_DBSCAN_rad[cluster]['Names'] = np.asarray(all_features_DBSCAN_rad[cluster]['Names'])\n",
    "    \n",
    "# DBSCAN PATH DISTANCE\n",
    "all_features_DBSCAN_path = {}\n",
    "for cluster in clusters_DBSCAN_path.keys():\n",
    "    curr_cluster_morphos = clusters_DBSCAN_path[cluster]\n",
    "    all_features_DBSCAN_path[cluster] = {'Names' : [], 'Vals' : []}\n",
    "    for morpho_name in curr_cluster_morphos:\n",
    "        curr_morpho_flags = morphometrics_neuron_names == morpho_name\n",
    "        if np.sum(curr_morpho_flags) > 0:\n",
    "            all_features_DBSCAN_path[cluster]['Vals'].append(morphometrics_feat_vals[curr_morpho_flags])\n",
    "            all_features_DBSCAN_path[cluster]['Names'].append(morphometrics_feat_names[curr_morpho_flags])\n",
    "        else:\n",
    "            print('Neuron not found in the morphometrics file!')\n",
    "    all_features_DBSCAN_path[cluster]['Vals'] = np.asarray(all_features_DBSCAN_path[cluster]['Vals'])\n",
    "    all_features_DBSCAN_path[cluster]['Names'] = np.asarray(all_features_DBSCAN_path[cluster]['Names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_features_GMM_rad.keys())\n",
    "print(all_features_GMM_path.keys())\n",
    "print(all_features_DBSCAN_rad.keys())\n",
    "print(all_features_DBSCAN_path.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = {'GMM_rad' : all_features_GMM_rad, 'GMM_path' : all_features_GMM_path, 'DBSCAN_rad' : all_features_DBSCAN_rad, 'DBSCAN_path' : all_features_DBSCAN_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hexadecimal color from seaborn palette\n",
    "pal = sns.color_palette('pastel')\n",
    "hex_colors = pal.as_hex()\n",
    "n_features = 21\n",
    "feat_names = all_features_DBSCAN_path[0]['Names'][0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### &rarr; Plotting GMM rad comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'GMM_rad'\n",
    "colors_to_use = hex_colors\n",
    "plt.figure(figsize = (25,10))\n",
    "for feat in range(n_features):\n",
    "    # We pool together all the clusters to normalize the current feature (need to put them together to keep the differences\n",
    "    # between clusters)\n",
    "    all_vals_for_norm = []\n",
    "    for cluster in all_features[model].keys():\n",
    "        all_vals_for_norm.append(all_features[model][cluster]['Vals'][:,feat])\n",
    "    all_vals_for_norm = np.concatenate(all_vals_for_norm)\n",
    "    mean_tmp = np.mean(all_vals_for_norm)\n",
    "    std_tmp = np.std(all_vals_for_norm)\n",
    "    \n",
    "    all_vals_clusters = []\n",
    "    for cluster in all_features[model].keys():\n",
    "        curr_feat_vals = all_features[model][cluster]['Vals'][:,feat]\n",
    "        norm_feat_vals = (curr_feat_vals - mean_tmp)/std_tmp\n",
    "        all_vals_clusters.append(norm_feat_vals)\n",
    "    plt.subplot(3,7,feat+1)\n",
    "    bp = plt.boxplot(all_vals_clusters, patch_artist = True)\n",
    "    # Parameters colors boxplot\n",
    "    # Customize athe face colornd outliers of each box\n",
    "    for patch, color in zip(bp['boxes'], colors_to_use):\n",
    "        patch.set_facecolor(color)\n",
    "    for patch, color in zip(bp['fliers'], colors_to_use):\n",
    "        patch.set_markerfacecolor(color)\n",
    "    plt.setp(bp['medians'], color = 'black')\n",
    "    # Axes etc\n",
    "    if feat > 13: plt.xticks(fontsize = 14, weight = 'bold')\n",
    "    plt.yticks(fontsize = 14)\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ylim1, ylim2 = ax.get_ylim()\n",
    "    plt.ylim([ylim1, ylim2 + 1])\n",
    "    if feat%7 == 0: plt.ylabel('Normalized feature', fontsize = 14, weight = 'bold')\n",
    "    if feat > 13: \n",
    "        plt.xticks(fontsize = 14, weight = 'bold')\n",
    "        plt.xlabel('Clusters', fontsize = 14, weight = 'bold')\n",
    "    else: \n",
    "        plt.xticks([])\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "    plt.title(feat_names[feat], fontsize = 14, weight = 'bold')\n",
    "\n",
    "plt.subplots_adjust(wspace = 0.4, hspace = 0.2)\n",
    "plt.savefig('../Results/Figures/New_morphometrics_%s.png'%model, dpi = 300, bbox_inches = 'tight')\n",
    "plt.savefig('../Results/Figures/New_morphometrics_%s.svg'%model, format = 'svg', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### &rarr; Plotting GMM path comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'GMM_path'\n",
    "colors_to_use = hex_colors\n",
    "plt.figure(figsize = (25,10))\n",
    "for feat in range(n_features):\n",
    "    # We pool together all the clusters to normalize the current feature (need to put them together to keep the differences\n",
    "    # between clusters)\n",
    "    all_vals_for_norm = []\n",
    "    for cluster in all_features[model].keys():\n",
    "        all_vals_for_norm.append(all_features[model][cluster]['Vals'][:,feat])\n",
    "    all_vals_for_norm = np.concatenate(all_vals_for_norm)\n",
    "    mean_tmp = np.mean(all_vals_for_norm)\n",
    "    std_tmp = np.std(all_vals_for_norm)\n",
    "    \n",
    "    all_vals_clusters = []\n",
    "    for cluster in all_features[model].keys():\n",
    "        curr_feat_vals = all_features[model][cluster]['Vals'][:,feat]\n",
    "        norm_feat_vals = (curr_feat_vals - mean_tmp)/std_tmp\n",
    "        all_vals_clusters.append(norm_feat_vals)\n",
    "    plt.subplot(3,7,feat+1)\n",
    "    bp = plt.boxplot(all_vals_clusters, patch_artist = True)\n",
    "    # Parameters colors boxplot\n",
    "    # Customize athe face colornd outliers of each box\n",
    "    for patch, color in zip(bp['boxes'], colors_to_use):\n",
    "        patch.set_facecolor(color)\n",
    "    for patch, color in zip(bp['fliers'], colors_to_use):\n",
    "        patch.set_markerfacecolor(color)\n",
    "    plt.setp(bp['medians'], color = 'black')\n",
    "    # Axes etc\n",
    "    if feat > 13: plt.xticks(fontsize = 14, weight = 'bold')\n",
    "    plt.yticks(fontsize = 14)\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ylim1, ylim2 = ax.get_ylim()\n",
    "    plt.ylim([ylim1, ylim2 + 1])\n",
    "    if feat%7 == 0: plt.ylabel('Normalized feature', fontsize = 14, weight = 'bold')\n",
    "    if feat > 13: \n",
    "        plt.xticks(fontsize = 14, weight = 'bold')\n",
    "        plt.xlabel('Clusters', fontsize = 14, weight = 'bold')\n",
    "    else: \n",
    "        plt.xticks([])\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "    plt.title(feat_names[feat], fontsize = 14, weight = 'bold')\n",
    "\n",
    "plt.subplots_adjust(wspace = 0.4, hspace = 0.2)\n",
    "plt.savefig('../Results/Figures/New_morphometrics_%s.png'%model, dpi = 300, bbox_inches = 'tight')\n",
    "plt.savefig('../Results/Figures/New_morphometrics_%s.svg'%model, format = 'svg', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### &rarr; Plotting DBSCAN rad comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'DBSCAN_rad'\n",
    "colors_to_use = hex_colors[1:]\n",
    "plt.figure(figsize = (25,10))\n",
    "for feat in range(n_features):\n",
    "    # We pool together all the clusters to normalize the current feature (need to put them together to keep the differences\n",
    "    # between clusters)\n",
    "    all_vals_for_norm = []\n",
    "    for cluster in all_features[model].keys():\n",
    "        all_vals_for_norm.append(all_features[model][cluster]['Vals'][:,feat])\n",
    "    all_vals_for_norm = np.concatenate(all_vals_for_norm)\n",
    "    mean_tmp = np.mean(all_vals_for_norm)\n",
    "    std_tmp = np.std(all_vals_for_norm)\n",
    "    \n",
    "    all_vals_clusters = []\n",
    "    for cluster in all_features[model].keys():\n",
    "        curr_feat_vals = all_features[model][cluster]['Vals'][:,feat]\n",
    "        norm_feat_vals = (curr_feat_vals - mean_tmp)/std_tmp\n",
    "        all_vals_clusters.append(norm_feat_vals)\n",
    "    plt.subplot(3,7,feat+1)\n",
    "    bp = plt.boxplot(all_vals_clusters, patch_artist = True)\n",
    "    # Parameters colors boxplot\n",
    "    # Customize athe face colornd outliers of each box\n",
    "    for patch, color in zip(bp['boxes'], colors_to_use):\n",
    "        patch.set_facecolor(color)\n",
    "    for patch, color in zip(bp['fliers'], colors_to_use):\n",
    "        patch.set_markerfacecolor(color)\n",
    "    plt.setp(bp['medians'], color = 'black')\n",
    "    # Axes etc\n",
    "    if feat > 13: plt.xticks(fontsize = 14, weight = 'bold')\n",
    "    plt.yticks(fontsize = 14)\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ylim1, ylim2 = ax.get_ylim()\n",
    "    plt.ylim([ylim1, ylim2 + 1])\n",
    "    if feat%7 == 0: plt.ylabel('Normalized feature', fontsize = 14, weight = 'bold')\n",
    "    if feat > 13: \n",
    "        plt.xticks(fontsize = 14, weight = 'bold')\n",
    "        plt.xlabel('Clusters', fontsize = 14, weight = 'bold')\n",
    "    else: \n",
    "        plt.xticks([])\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "    plt.title(feat_names[feat], fontsize = 14, weight = 'bold')\n",
    "\n",
    "plt.subplots_adjust(wspace = 0.4, hspace = 0.2)\n",
    "plt.savefig('../Results/Figures/New_morphometrics_%s.png'%model, dpi = 300, bbox_inches = 'tight')\n",
    "plt.savefig('../Results/Figures/New_morphometrics_%s.svg'%model, format = 'svg', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### &rarr; Plotting DBSCAN path comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'DBSCAN_path'\n",
    "colors_to_use = hex_colors[1:]\n",
    "plt.figure(figsize = (25,10))\n",
    "for feat in range(n_features):\n",
    "    # We pool together all the clusters to normalize the current feature (need to put them together to keep the differences\n",
    "    # between clusters)\n",
    "    all_vals_for_norm = []\n",
    "    for cluster in all_features[model].keys():\n",
    "        all_vals_for_norm.append(all_features[model][cluster]['Vals'][:,feat])\n",
    "    all_vals_for_norm = np.concatenate(all_vals_for_norm)\n",
    "    mean_tmp = np.mean(all_vals_for_norm)\n",
    "    std_tmp = np.std(all_vals_for_norm)\n",
    "    \n",
    "    all_vals_clusters = []\n",
    "    for cluster in all_features[model].keys():\n",
    "        curr_feat_vals = all_features[model][cluster]['Vals'][:,feat]\n",
    "        norm_feat_vals = (curr_feat_vals - mean_tmp)/std_tmp\n",
    "        all_vals_clusters.append(norm_feat_vals)\n",
    "    plt.subplot(3,7,feat+1)\n",
    "    bp = plt.boxplot(all_vals_clusters, patch_artist = True)\n",
    "    # Parameters colors boxplot\n",
    "    # Customize athe face colornd outliers of each box\n",
    "    for patch, color in zip(bp['boxes'], colors_to_use):\n",
    "        patch.set_facecolor(color)\n",
    "    for patch, color in zip(bp['fliers'], colors_to_use):\n",
    "        patch.set_markerfacecolor(color)\n",
    "    plt.setp(bp['medians'], color = 'black')\n",
    "    # Axes etc\n",
    "    if feat > 13: plt.xticks(fontsize = 14, weight = 'bold')\n",
    "    plt.yticks(fontsize = 14)\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ylim1, ylim2 = ax.get_ylim()\n",
    "    plt.ylim([ylim1, ylim2 + 3])\n",
    "    if feat%7 == 0: plt.ylabel('Normalized feature', fontsize = 14, weight = 'bold')\n",
    "    if feat > 13: \n",
    "        plt.xticks(fontsize = 14, weight = 'bold')\n",
    "        plt.xlabel('Clusters', fontsize = 14, weight = 'bold')\n",
    "    else: \n",
    "        plt.xticks([])\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "    plt.title(feat_names[feat], fontsize = 14, weight = 'bold')\n",
    "\n",
    "plt.subplots_adjust(wspace = 0.4, hspace = 0.2)\n",
    "plt.savefig('../Results/Figures/New_morphometrics_%s.png'%model, dpi = 300, bbox_inches = 'tight')\n",
    "plt.savefig('../Results/Figures/New_morphometrics_%s.svg'%model, format = 'svg', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### &rarr; t-test since there are only two clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'GMM_rad'\n",
    "print('========= PERFORMING INDEPENDENT T-TESTS BETWEEN 2 CLUSTERS =========')\n",
    "for feat in range(n_features):\n",
    "    # We pool together all the clusters to normalize the current feature (need to put them together to keep the differences\n",
    "    # between clusters)\n",
    "    all_vals_for_norm = []\n",
    "    for cluster in all_features[model].keys():\n",
    "        all_vals_for_norm.append(all_features[model][cluster]['Vals'][:,feat])\n",
    "    all_vals_for_norm = np.concatenate(all_vals_for_norm)\n",
    "    mean_tmp = np.mean(all_vals_for_norm)\n",
    "    std_tmp = np.std(all_vals_for_norm)\n",
    "    \n",
    "    all_vals_clusters = []\n",
    "    for cluster in all_features[model].keys():\n",
    "        curr_feat_vals = all_features[model][cluster]['Vals'][:,feat]\n",
    "        norm_feat_vals = (curr_feat_vals - mean_tmp)/std_tmp\n",
    "        all_vals_clusters.append(norm_feat_vals)\n",
    "        \n",
    "    tmp_t, tmp_p = ttest_ind(all_vals_clusters[0], all_vals_clusters[1])\n",
    "    if tmp_p < 0.01: print('%d - Feature %s : SIGNIFICANT (p = %0.8f)'%(feat+1, feat_names[feat], tmp_p))\n",
    "    else: print('%d - Feature %s : non significant (p = %0.8f)'%(feat+1, feat_names[feat], tmp_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### &rarr; ANOVA and post-hoc tests for multiple clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'DBSCAN_path'\n",
    "print('========= PERFORMING ANOVA =========')\n",
    "for feat in range(n_features):\n",
    "    print('============================================= %s ============================================='%feat_names[feat])\n",
    "    # We pool together all the clusters to normalize the current feature (need to put them together to keep the differences\n",
    "    # between clusters)\n",
    "    all_vals_for_norm = []\n",
    "    for cluster in all_features[model].keys():\n",
    "        all_vals_for_norm.append(all_features[model][cluster]['Vals'][:,feat])\n",
    "    all_vals_for_norm = np.concatenate(all_vals_for_norm)\n",
    "    mean_tmp = np.mean(all_vals_for_norm)\n",
    "    std_tmp = np.std(all_vals_for_norm)\n",
    "    \n",
    "    all_vals_clusters = []\n",
    "    all_vals_groups = []\n",
    "    for c, cluster in enumerate(all_features[model].keys()):\n",
    "        curr_feat_vals = all_features[model][cluster]['Vals'][:,feat]\n",
    "        norm_feat_vals = (curr_feat_vals - mean_tmp)/std_tmp\n",
    "        all_vals_clusters.append(norm_feat_vals)\n",
    "        all_vals_groups.append([c+1]*len(curr_feat_vals))\n",
    "        \n",
    "    tmp_f, tmp_p = f_oneway(*all_vals_clusters)\n",
    "    if tmp_p > 0.01 : \n",
    "        print('%d - Fisher test: non significant (p = %0.8f)'%(feat+1, tmp_p))\n",
    "    else: \n",
    "        print('%d - Fisher test: SIGNIFICANT (p = %0.8f)'%(feat+1, tmp_p))\n",
    "        tukey_results = pairwise_tukeyhsd(np.concatenate(all_vals_clusters), np.concatenate(all_vals_groups), alpha = 0.01)\n",
    "        print(tukey_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
