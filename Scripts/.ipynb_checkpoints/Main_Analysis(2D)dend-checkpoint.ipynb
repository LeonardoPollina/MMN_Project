{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nvme/kurban/1518813/ipykernel_280417/1422133931.py:25: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(['seaborn-white'])\n"
     ]
    }
   ],
   "source": [
    "import neurom as nm\n",
    "import plotly.express as px\n",
    "import morphio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from scipy.spatial import distance_matrix\n",
    "import gudhi as gd \n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import tmd\n",
    "import os\n",
    "import cv2\n",
    "from tmd.view import plot as tmdplt\n",
    "from tmd.Topology import analysis\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from Utilities import *\n",
    "plt.style.use(['seaborn-white'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### General variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "morpho_data_path = '../data/morphologies/swc30/'\n",
    "metadata_data_path = '../data/metadata/'\n",
    "thresholds_occur_regs = 30\n",
    "random_state = 1441"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loading of metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the CSV file containing brain regions into a pandas dataframe\n",
    "df = pd.read_csv(metadata_data_path + 'morph_regions.csv')\n",
    "morpho_labels_reg = df.iloc[:,0].to_numpy()\n",
    "regions_labels = df.iloc[:,1].to_numpy()\n",
    "regions_dict = {}\n",
    "for m in range(len(morpho_labels_reg)):\n",
    "    regions_dict[morpho_labels_reg[m]] = regions_labels[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file containing m_types regions into a pandas dataframe\n",
    "df = pd.read_csv(metadata_data_path + 'morpho_metadata_raw.csv')\n",
    "df = df[['neuron_name', 'cell_type']]\n",
    "morpho_names_tmp = df.iloc[:,0].to_numpy()\n",
    "morpho_names_m_types = np.asarray([morpho_name + '.swc' for morpho_name in morpho_names_tmp])\n",
    "m_types = df.iloc[:,1].to_numpy()\n",
    "\n",
    "# We select only those m_types appearing more than 10 times in the dataset\n",
    "unique_m_types, counts_m_types = np.unique(m_types, return_counts = True)\n",
    "m_types_OI = unique_m_types[counts_m_types > 10]\n",
    "\n",
    "# We look for the indices of the morphologies we are interested in keeping and we store them\n",
    "idx_morphos_OI = []\n",
    "for m_type in m_types_OI:\n",
    "    idx_morphos_OI.append(np.where(m_types == m_type)[0])\n",
    "idx_morphos_OI = np.concatenate(idx_morphos_OI)\n",
    "\n",
    "# We built a final dictionary mapping the name of a morphology to its m_type\n",
    "morpho_names_m_types_OI = morpho_names_m_types[idx_morphos_OI]\n",
    "m_types_OI_final = m_types[idx_morphos_OI]\n",
    "m_types_dict = {}\n",
    "for m in range(len(morpho_names_m_types_OI)):\n",
    "    m_types_dict[morpho_names_m_types_OI[m]] = m_types_OI_final[m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loading morphologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load the file.\n",
      "Could not load the file.\n",
      "Could not load the file.\n",
      "Could not load the file.\n",
      "1103 morphologies were loaded.\n"
     ]
    }
   ],
   "source": [
    "# We now load all the actual morphologies from the .swc files.\n",
    "# Not all files can be loaded and the regions and m_types are not necessarily in the same order as we load them but we use \n",
    "# the dictionaries we built to map them and store these info in the good order\n",
    "list_morpho = os.listdir(morpho_data_path)\n",
    "all_morphos = []\n",
    "all_names = []\n",
    "all_regs = []\n",
    "all_m_types = []\n",
    "for i, tmp_morpho in enumerate(m_types_dict.keys()):\n",
    "    if tmp_morpho.endswith('.swc'):\n",
    "        tmp_filename = morpho_data_path + tmp_morpho\n",
    "        try: \n",
    "            all_morphos.append(tmd.io.load_neuron_from_morphio(tmp_filename))\n",
    "            all_names.append(tmp_morpho)\n",
    "            all_regs.append(regions_dict[tmp_morpho])\n",
    "            all_m_types.append(m_types_dict[tmp_morpho])\n",
    "        except: print('Could not load the file.')\n",
    "print('%d morphologies were loaded.'%len(all_morphos))\n",
    "all_morphos = np.asarray(all_morphos)\n",
    "all_names = np.asarray(all_names)\n",
    "all_regs = np.asarray(all_regs)\n",
    "all_m_types = np.asarray(all_m_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the regions corresponding to the m_type 'granule' are changed to DG (after checking that it is indeed the case)\n",
    "# If we don't do this this m_type would be discarded since it does not appear in the same exact region more than the threshold \n",
    "# impose\n",
    "for m, m_type in enumerate(np.unique(all_m_types)):\n",
    "    if m_type.find('granule') != -1:\n",
    "        id_granule = m\n",
    "all_regs[all_m_types == np.unique(all_m_types)[id_granule]] = 'DG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the regions corresponding to the m_type 'Purkinje' are changed to Cerebellum (after checking that it is indeed the case)\n",
    "# If we don't do this this m_type would be discarded since it does not appear in the same exact region more than the threshold \n",
    "# impose\n",
    "for m, m_type in enumerate(np.unique(all_m_types)):\n",
    "    if m_type.find('Purkinje') != -1:\n",
    "        id_purkinje = m\n",
    "all_regs[all_m_types == np.unique(all_m_types)[id_purkinje]] = 'Cerebellum'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Getting only the axon neurite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each morphology loaded, we select only the axonal part since the focus of our analysis\n",
    "all_morpho_axons = [morpho.axon[0] for morpho in all_morphos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Selecting only the samples (morphologies) belonging to regions with a certain number of occurencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 regions passing the threshold over the minimal number of occurencies.\n",
      "There regions are : ['Cerebellum' 'DG' 'MOs2/3' 'MOs5' 'MOs6a' 'PRE' 'SUB' 'VAL']\n"
     ]
    }
   ],
   "source": [
    "unique_regions, counts_regions = np.unique(all_regs, return_counts = True)\n",
    "regions_desired = unique_regions[counts_regions > thresholds_occur_regs]\n",
    "print('There are %d regions passing the threshold over the minimal number of occurencies.'%len(regions_desired))\n",
    "print('There regions are :', regions_desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We store the indices of the morphologies belonging to the regions of interest identified \n",
    "idx_regions_desired = {}\n",
    "for reg in regions_desired:\n",
    "    idx_regions_desired[reg] = np.where(np.asarray(all_regs) == reg)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of morphos :  512\n",
      "Number of file names :  512\n",
      "Number of m_types :  512\n",
      "Number of regs :  512\n"
     ]
    }
   ],
   "source": [
    "# We rebuild the variables that we want by selecting now only the morphologies belong to the regions OI\n",
    "morphos_OI = []\n",
    "regs_OI = []\n",
    "names_OI = []\n",
    "m_types_OI = []\n",
    "for reg in idx_regions_desired.keys():\n",
    "    morphos_OI.append(np.asarray(all_morpho_axons)[idx_regions_desired[reg]])\n",
    "    regs_OI.append([reg] * len(idx_regions_desired[reg]))\n",
    "    names_OI.append(np.asarray(all_names)[idx_regions_desired[reg]])\n",
    "    m_types_OI.append(np.asarray(all_m_types)[idx_regions_desired[reg]])\n",
    "morphos_OI = np.asarray(np.concatenate(morphos_OI))\n",
    "regs_OI = np.asarray(np.concatenate(regs_OI))\n",
    "names_OI = np.asarray(np.concatenate(names_OI))\n",
    "m_types_OI = np.asarray(np.concatenate(m_types_OI))\n",
    "n_morphos = len(morphos_OI)\n",
    "print('Number of morphos : ', n_morphos)\n",
    "print('Number of file names : ', len(names_OI))\n",
    "print('Number of m_types : ', len(m_types_OI))\n",
    "print('Number of regs : ', len(regs_OI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computing the persistent diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute now the persistent diagrams of the axonal tress of the selected morphologies.\n",
    "# We do so by computing it both by using the 'radial distance' and the 'path distance' as filtration metrics\n",
    "all_pd = {'rad' : [], 'path' : []}\n",
    "for axon_tree in morphos_OI:\n",
    "    all_pd['rad'].append(tmd.methods.get_persistence_diagram(axon_tree, feature = \"radial_distances\"))\n",
    "    all_pd['path'].append(tmd.methods.get_persistence_diagram(axon_tree, feature = \"path_distances\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computing the persistent images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persistent image has not been computed.\n",
      "Persistent image has not been computed.\n",
      "Persistent image has not been computed.\n",
      "Now there are 511 neurons.\n",
      "510\n",
      "510\n",
      "510\n"
     ]
    }
   ],
   "source": [
    "# We compute the persistent images by using the persistent diagrams we already computed. \n",
    "# - We are scaling all the persistent images to the same scales (xlims, ylims)\n",
    "# - We are using as norm_factor 1 in order to NOT normalize for now the persistent images\n",
    "all_pi = {'rad': [], 'path' : []}\n",
    "not_computed_idx = []\n",
    "for metric in all_pd.keys():\n",
    "    # Computing the xlim and ylim in order to scale all the persistent images accordingly\n",
    "    xlims, ylims = analysis.get_limits(all_pd[metric])\n",
    "    for i, tmp_pd in enumerate(all_pd[metric]):\n",
    "        try: \n",
    "            # We set norm factor to 1 because we actually don't want to normalize the persistent images individually\n",
    "            all_pi[metric].append(analysis.get_persistence_image_data(tmp_pd, xlim = xlims, ylim = ylims, norm_factor = 1))\n",
    "        except: \n",
    "            print('Persistent image has not been computed.')\n",
    "            not_computed_idx.append(i)\n",
    "names_OI = np.delete(names_OI, np.unique(not_computed_idx))\n",
    "m_types_OI = np.delete(m_types_OI, np.unique(not_computed_idx))\n",
    "regs_OI = np.delete(regs_OI, np.unique(not_computed_idx))\n",
    "n_morphos = len(names_OI)\n",
    "print('Now there are %d neurons.'%np.shape(all_pi[metric])[0])\n",
    "print(len(names_OI))\n",
    "print(len(m_types_OI))\n",
    "print(len(regs_OI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nvme/kurban/1518813/ipykernel_280417/452567775.py:9: RuntimeWarning: invalid value encountered in divide\n",
      "  all_pi_norm[metric].append(n_branches * all_pi[metric][im]/np.sum(all_pi[metric][im])) # norm by sum and mult by n_branches\n"
     ]
    }
   ],
   "source": [
    "# To normalize we divide each persistent images by the sum of its values in the picture (this makes each image a sort of \n",
    "# probabilistic map). Then we multiply them per the number of branches there were in the diagram (in order to take into account \n",
    "# the differences in the number of branches).\n",
    "all_pi_norm = {}\n",
    "for metric in all_pi.keys():\n",
    "    all_pi_norm[metric] = []\n",
    "    for im in range(len(all_pi[metric])):\n",
    "        n_branches = len(all_pd[metric][im])\n",
    "        all_pi_norm[metric].append(n_branches * all_pi[metric][im]/np.sum(all_pi[metric][im])) # norm by sum and mult by n_branches\n",
    "    all_pi_norm[metric] = np.asarray(all_pi_norm[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(510, 100, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pi_norm['rad'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_variable(all_pi_norm, '../Results/Saved_variables/all_persistent_images_normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### TO CHECK (NOT SURE)\n",
    "# # This is just to plot the persistent images all on the same scale and also normalized as explained above\n",
    "# for metric in all_pd.keys():\n",
    "#     # Computing the xlim and ylim in order to scale all the persistent images accordingly\n",
    "#     xlims, ylims = analysis.get_limits(all_pd[metric])\n",
    "#     for i, tmp_pd in enumerate(all_pd[metric]):\n",
    "#         n_branches = len(tmp_pd)\n",
    "#         norm_fact = np.sum(all_pi[metric][im])/n_branches\n",
    "#         vmax = np.max(all_pi[metric][im]/norm_fact)\n",
    "# #         tmp , _ = tmdplt.persistence_image(tmp_pd, xlim = xlims, ylim = ylims, vmin = 0, vmax = 1, norm_factor = norm_fact)\n",
    "#         try: \n",
    "#             tmp , _ = tmdplt.persistence_image(tmp_pd, xlim = xlims, ylim = ylims, vmin = 0, vmax = vmax, norm_factor = norm_fact)\n",
    "#             plt.savefig('../Results/Persistent_images/Scaled_images/%s/%s.png'%(metric, names_OI[i].split('.')[0]), bbox_inches = 'tight')\n",
    "#         except: \n",
    "#             print('Persistent image has not been computed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Merging the subregions of MO and make them all 'MO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new (merged) regions are:\n",
      "- Cerebellum\n",
      "- DG\n",
      "- MO\n",
      "- PRE\n",
      "- SUB\n",
      "- VAL\n"
     ]
    }
   ],
   "source": [
    "merged_regs = []\n",
    "for reg in regs_OI:\n",
    "    if reg[:2] == 'MO': merged_regs.append('MO')\n",
    "    else: merged_regs.append(reg)\n",
    "merged_regs = np.asarray(merged_regs)\n",
    "print('The new (merged) regions are:')\n",
    "for i in np.unique(merged_regs):\n",
    "    print('- %s'%i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Restructuring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We flatten the persistent images in order to have a final matrix having shape N x F, where N = number of samples (neurons) and\n",
    "# F = number of features\n",
    "flatten_pi = {}\n",
    "for metric in all_pi_norm.keys():\n",
    "    tmp_data = all_pi_norm[metric]\n",
    "    n_samples = np.shape(tmp_data)[0]\n",
    "    flatten_pi[metric] = np.reshape(tmp_data, (n_samples, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Applying dimensionality reduction (tSNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nTSNE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m flatten_pi\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      3\u001b[0m     tmp_data \u001b[38;5;241m=\u001b[39m flatten_pi[metric]\n\u001b[0;32m----> 4\u001b[0m     data_embedded[metric] \u001b[38;5;241m=\u001b[39m \u001b[43mTSNE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperplexity\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_morphos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/gpfs/bbp.cscs.ch/project/proj142/home/kurban/venv2023/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:1119\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m-> 1119\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_\n",
      "File \u001b[0;32m/gpfs/bbp.cscs.ch/project/proj142/home/kurban/venv2023/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:854\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbarnes_hut\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 854\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    861\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    862\u001b[0m         X, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mfloat64]\n\u001b[1;32m    863\u001b[0m     )\n",
      "File \u001b[0;32m/gpfs/bbp.cscs.ch/project/proj142/home/kurban/venv2023/lib/python3.8/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m/gpfs/bbp.cscs.ch/project/proj142/home/kurban/venv2023/lib/python3.8/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m/gpfs/bbp.cscs.ch/project/proj142/home/kurban/venv2023/lib/python3.8/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nTSNE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "data_embedded = {}\n",
    "for metric in flatten_pi.keys():\n",
    "    tmp_data = flatten_pi[metric]\n",
    "    data_embedded[metric] = TSNE(n_components = 2, perplexity = np.round(np.sqrt(n_morphos)), random_state = random_state).fit_transform(tmp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_variable(data_embedded, '../Results/Saved_variables/data_embedded_TSNE_n2_random_state%d'%random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We build a labels vector for different types of labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original regions of interested (not merged MO layers)\n",
    "labels_regs = np.unique(regs_OI)\n",
    "idx_labels_regs = {}\n",
    "for label in labels_regs:\n",
    "    idx_labels_regs[label] = np.where(regs_OI == label)[0]\n",
    "\n",
    "# Merged regions labels\n",
    "labels_merged_regs = np.unique(merged_regs)\n",
    "idx_labels_merged_regs = {}\n",
    "for label in labels_merged_regs:\n",
    "    idx_labels_merged_regs[label] = np.where(merged_regs == label)[0]\n",
    "\n",
    "# M-types layers\n",
    "labels_m_types = np.unique(m_types_OI)\n",
    "idx_labels_m_types = {}\n",
    "for label in labels_m_types:\n",
    "    idx_labels_m_types[label] = np.where(m_types_OI == label)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plotting t-SNE visualization following different labels or metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_labels = idx_labels_regs\n",
    "plt.figure(figsize = (20, 8))\n",
    "sns.set_palette(\"pastel\")\n",
    "for m, metric in enumerate(data_embedded.keys()):\n",
    "    tmp_embedded_data = data_embedded[metric]\n",
    "    all_data = []\n",
    "    plt.subplot(1,2,m+1)\n",
    "    for lab in curr_labels.keys():\n",
    "        x = tmp_embedded_data[curr_labels[lab],0]\n",
    "        y = tmp_embedded_data[curr_labels[lab],1]\n",
    "        if m == 1: sns.scatterplot(x,y, label = lab, s = 100)\n",
    "        else: sns.scatterplot(x,y,s = 100)\n",
    "    if m == 1: plt.legend(fontsize = 14, bbox_to_anchor = (1.15,1))\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.xlabel('1st Component', fontsize = 18, weight = 'bold')\n",
    "    plt.ylabel('2nd Component', fontsize = 18, weight = 'bold')\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.title(metric, weight = 'bold', fontsize = 24)\n",
    "plt.savefig('../Results/Figures/TSNE_proj_n2_regs.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_labels = idx_labels_merged_regs\n",
    "plt.figure(figsize = (20, 8))\n",
    "sns.set_palette(\"pastel\")\n",
    "for m, metric in enumerate(data_embedded.keys()):\n",
    "    tmp_embedded_data = data_embedded[metric]\n",
    "    all_data = []\n",
    "    plt.subplot(1,2,m+1)\n",
    "    for lab in curr_labels.keys():\n",
    "        x = tmp_embedded_data[curr_labels[lab],0]\n",
    "        y = tmp_embedded_data[curr_labels[lab],1]\n",
    "        if m == 1: sns.scatterplot(x,y, label = lab, s = 100)\n",
    "        else: sns.scatterplot(x,y,s = 100)\n",
    "    if m == 1: plt.legend(fontsize = 14, bbox_to_anchor = (1.15,1))\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.xlabel('1st Component', fontsize = 18, weight = 'bold')\n",
    "    plt.ylabel('2nd Component', fontsize = 18, weight = 'bold')\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.title(metric, weight = 'bold', fontsize = 24)\n",
    "plt.savefig('../Results/Figures/TSNE_proj_n2_merged_regs.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_labels = idx_labels_m_types \n",
    "plt.figure(figsize = (20, 8))\n",
    "sns.set_palette(\"pastel\")\n",
    "for m, metric in enumerate(data_embedded.keys()):\n",
    "    tmp_embedded_data = data_embedded[metric]\n",
    "    all_data = []\n",
    "    plt.subplot(1,2,m+1)\n",
    "    for lab in curr_labels.keys():\n",
    "        x = tmp_embedded_data[curr_labels[lab],0]\n",
    "        y = tmp_embedded_data[curr_labels[lab],1]\n",
    "        if m == 1: sns.scatterplot(x,y, label = lab, s = 100)\n",
    "        else: sns.scatterplot(x,y,s = 100)\n",
    "    if m == 1: plt.legend(fontsize = 14, bbox_to_anchor = (1.05,1))\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.xlabel('1st Component', fontsize = 18, weight = 'bold')\n",
    "    plt.ylabel('2nd Component', fontsize = 18, weight = 'bold')\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.title(metric, weight = 'bold', fontsize = 24)\n",
    "plt.savefig('../Results/Figures/TSNE_proj_n2_mtypes.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now selecting only the neurons belonging to the m_type pyramidal or projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select the two classes of m_types that we see overlap a lot in the projection using t-SNE \n",
    "mt1 = \"['principal cell', 'projection']\"\n",
    "mt2 = \"['principal cell', 'pyramidal', 'projection']\"\n",
    "\n",
    "idx1 = np.where(m_types_OI == mt1)[0]\n",
    "idx2 = np.where(m_types_OI == mt2)[0]\n",
    "all_idx_OI = np.concatenate([idx1, idx2])\n",
    "sel_names = names_OI[all_idx_OI]\n",
    "n_sel_morphos = len(all_idx_OI)\n",
    "print('There are %s selected morphos.'%n_sel_morphos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embedded_sel = {}\n",
    "for metric in data_embedded.keys():\n",
    "    data_embedded_sel[metric] = data_embedded[metric][all_idx_OI, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_variable(data_embedded_sel, '../Results/Saved_variables/data_embedded_selected_morphos_TSNE_n2_random_state%d'%random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_labels = idx_labels_m_types \n",
    "plt.figure(figsize = (20, 8))\n",
    "sns.set_palette(\"Blues\")\n",
    "for m, metric in enumerate(data_embedded_sel.keys()):\n",
    "    tmp_embedded_data = data_embedded_sel[metric]\n",
    "    all_data = []\n",
    "    plt.subplot(1,2,m+1)\n",
    "    x = tmp_embedded_data[:,0]\n",
    "    y = tmp_embedded_data[:,1]\n",
    "    sns.scatterplot(x,y,s = 100)\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.yticks(fontsize = 14)\n",
    "    plt.xlabel('1st Component', fontsize = 18, weight = 'bold')\n",
    "    plt.ylabel('2nd Component', fontsize = 18, weight = 'bold')\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.title(metric, weight = 'bold', fontsize = 24)\n",
    "plt.savefig('../Results/Figures/TSNE_proj_n2_selected_morphos.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Trying to cluster the projected morphos using  K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We try to optimize the number of clusters using the elbow method \n",
    "# Also, because of randomness present in K-means initialization, we repeat the procedure 20 times and we plot only the average of\n",
    "# the sse values  (Sum of Squared Errors)\n",
    "k_values = range(1, 11)  # Possible number of clusters\n",
    "all_sse = {}\n",
    "for metric in data_embedded_sel.keys():\n",
    "    all_sse[metric] = []\n",
    "    for rep in range(20):\n",
    "        tmp_data = data_embedded_sel[metric]\n",
    "        sse = []\n",
    "        for k in k_values:\n",
    "            kmeans = KMeans(n_clusters = k, n_init = 'auto')\n",
    "            kmeans.fit(tmp_data)\n",
    "            sse.append(kmeans.inertia_)\n",
    "        all_sse[metric].append(sse)\n",
    "    all_sse[metric] = np.asarray(all_sse[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing mean and std over the 20 repetitions\n",
    "sse_mean_rad = np.mean(all_sse['rad'], axis = 0)\n",
    "sse_std_rad = np.std(all_sse['rad'], axis = 0)\n",
    "sse_mean_path = np.mean(all_sse['path'], axis = 0)\n",
    "sse_std_path = np.std(all_sse['path'], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7,5))\n",
    "# RADIAL\n",
    "plt.plot(k_values, sse_mean_rad, ls = '-', lw = 4, marker = '.', ms = 20, label = 'Radial distance')\n",
    "plt.plot(k_values, sse_mean_path, ls = '-', lw = 4, marker = '.', ms = 20, label = 'Path distance')\n",
    "plt.xlabel('Number of Clusters (K)', fontsize = 14, weight = 'bold')\n",
    "plt.ylabel('Sum of Squared Distances', fontsize = 14, weight = 'bold')\n",
    "plt.title('Elbow method', fontsize = 18, weight = 'bold')\n",
    "plt.xticks(k_values, fontsize = 14, weight = 'bold')\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.legend(fontsize = 18, bbox_to_anchor = (1.05,1))\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.savefig('../Results/Figures/Elbow_method_opt_kmeans.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "plt.figure(figsize = (20, 8))\n",
    "sns.set_palette(\"Blues\")\n",
    "for m, metric in enumerate(data_embedded_sel.keys()):\n",
    "    plt.subplot(1,2,m+1)\n",
    "    kmeans = KMeans(n_clusters = k, n_init = 'auto', random_state = random_state).fit(data_embedded_sel[metric])\n",
    "    labels = kmeans.labels_\n",
    "    for l in np.unique(labels):\n",
    "        x = data_embedded_sel[metric][labels == l,0]\n",
    "        y = data_embedded_sel[metric][labels == l,1]\n",
    "        sns.scatterplot(x,y, s = 100, label = 'Cluster %d'%l)\n",
    "        plt.legend(fontsize = 18)\n",
    "        plt.xticks(fontsize = 14)\n",
    "        plt.yticks(fontsize = 14)\n",
    "        plt.xlabel('1st Component', fontsize = 18, weight = 'bold')\n",
    "        plt.ylabel('2nd Component', fontsize = 18, weight = 'bold')\n",
    "        ax = plt.gca()\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        plt.title(metric, weight = 'bold', fontsize = 24)\n",
    "plt.savefig('../Results/Figures/Cluster_kmeans.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Trying the clustering via Gaussia Mixture Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We try to optimize the number of clusters for GMM using the Silhouette Score\n",
    "k_values = range(2, 11)  # Possible number of clusters\n",
    "silhouette_scores = {}\n",
    "for metric in data_embedded_sel.keys():\n",
    "    silhouette_scores[metric] = []\n",
    "    for rep in range(20):\n",
    "        tmp_data = data_embedded_sel[metric]\n",
    "        scores = []\n",
    "        for k in k_values:\n",
    "            gmm = GaussianMixture(n_components=k)\n",
    "            labels = gmm.fit_predict(tmp_data)\n",
    "            scores.append(silhouette_score(tmp_data, labels))\n",
    "        silhouette_scores[metric].append(scores)\n",
    "    silhouette_scores[metric] = np.asarray(silhouette_scores[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (9,5))\n",
    "for metric in silhouette_scores.keys():\n",
    "    plt.plot(k_values, np.mean(silhouette_scores[metric], axis = 0), lw = 4, label = metric)\n",
    "plt.legend(fontsize = 18)\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.xticks(fontsize = 18, weight = 'bold')\n",
    "plt.yticks(fontsize = 18)\n",
    "plt.xlabel('N. clusters', fontsize = 18, weight = 'bold')\n",
    "plt.ylabel('Silhouette Score', fontsize = 18, weight = 'bold')\n",
    "plt.savefig('../Results/Figures/Silhouette_GMMs.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "plt.figure(figsize = (20, 8))\n",
    "sns.set_palette(\"Blues\")\n",
    "for m, metric in enumerate(data_embedded_sel.keys()):\n",
    "    plt.subplot(1,2,m+1)\n",
    "    gmm = GaussianMixture(n_components=k, random_state = random_state)\n",
    "    labels = gmm.fit_predict(data_embedded_sel[metric])\n",
    "    for l in np.unique(labels):\n",
    "        x = data_embedded_sel[metric][labels == l,0]\n",
    "        y = data_embedded_sel[metric][labels == l,1]\n",
    "        sns.scatterplot(x,y, s = 100, label = 'Cluster %d'%l)\n",
    "        plt.legend(fontsize = 18)\n",
    "        plt.xticks(fontsize = 14)\n",
    "        plt.yticks(fontsize = 14)\n",
    "        plt.xlabel('1st Component', fontsize = 18, weight = 'bold')\n",
    "        plt.ylabel('2nd Component', fontsize = 18, weight = 'bold')\n",
    "        ax = plt.gca()\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        plt.title(metric, weight = 'bold', fontsize = 24)\n",
    "plt.savefig('../Results/Figures/Cluster_GMMs.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computing how much the clusters found using Path Distance and Radial Distance overlap (K-Means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "kmeans_rad = KMeans(n_clusters = k, n_init = 'auto', random_state = random_state).fit(data_embedded_sel['rad'])\n",
    "labels_kmeans_rad = kmeans_rad.labels_\n",
    "kmeans_path = KMeans(n_clusters = k, n_init = 'auto', random_state = random_state).fit(data_embedded_sel['path'])\n",
    "labels_kmeans_path = kmeans_path.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_rad = {}\n",
    "for lab in np.unique(labels_kmeans_rad):\n",
    "    sets_rad[lab] = set(np.where(labels_kmeans_rad == lab)[0])\n",
    "sets_path = {}\n",
    "for lab in np.unique(labels_kmeans_path):\n",
    "    sets_path[lab] = set(np.where(labels_kmeans_path == lab)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_intersec = []\n",
    "for set1 in sets_rad.keys():\n",
    "    for set2 in sets_path.keys():\n",
    "        inter = sets_rad[set1].intersection(sets_path[set2])\n",
    "        vals_intersec.append(100* len(inter)/len(sets_rad[set1]))\n",
    "vals_intersec = np.asarray(vals_intersec)\n",
    "vals_intersec = np.reshape(vals_intersec, (k,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,6))\n",
    "plt.imshow(vals_intersec, aspect = 'auto', cmap = 'Blues', vmin = 0, vmax = 100)\n",
    "\n",
    "# Labels\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(k))\n",
    "ax.set_yticks(np.arange(k))\n",
    "ax.set_xticklabels(range(k), fontsize = 18, weight = 'bold')\n",
    "ax.set_yticklabels(range(k), fontsize = 18, weight = 'bold')\n",
    "plt.setp(ax.get_xticklabels(), ha = 'right', rotation_mode = 'anchor')\n",
    "plt.ylabel('Radial', fontsize = 24, weight = 'bold')\n",
    "plt.xlabel('Path', fontsize = 24, weight = 'bold')\n",
    "\n",
    "# Loop over data dimensions and create text annotations to show the percentage of percentage of predictions for each combination\n",
    "for i in range(k):\n",
    "    for j in range(k):\n",
    "        if vals_intersec[i, j] > 50:\n",
    "            text = ax.text(j, i, str(round(vals_intersec[i, j],2)) + '%', ha = 'center', va = 'center', color = 'white', fontsize = 18)\n",
    "        if vals_intersec[i, j] < 50:\n",
    "            text = ax.text(j, i, str(round(vals_intersec[i, j],2)) + '%', ha = 'center', va = 'center', color = 'black', fontsize = 18)\n",
    "\n",
    "plt.title('Intersection in \\nidentified clusters', fontsize = 28, weight = 'bold')         \n",
    "plt.savefig('../Results/Figures/Perce_intersec_clusters_kmeans.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computing how much the clusters found using Path Distance and Radial Distance overlap (GMMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "gmm = GaussianMixture(n_components = k, random_state = random_state)\n",
    "labels_gmm_rad = gmm.fit_predict(data_embedded_sel['rad'])\n",
    "gmm = GaussianMixture(n_components = k, random_state = random_state)\n",
    "labels_gmm_path = gmm.fit_predict(data_embedded_sel['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_rad = {}\n",
    "for lab in np.unique(labels_gmm_rad):\n",
    "    sets_rad[lab] = set(np.where(labels_gmm_rad == lab)[0])\n",
    "sets_path = {}\n",
    "for lab in np.unique(labels_gmm_path):\n",
    "    sets_path[lab] = set(np.where(labels_gmm_path == lab)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_intersec = []\n",
    "for set1 in sets_rad.keys():\n",
    "    for set2 in sets_path.keys():\n",
    "        inter = sets_rad[set1].intersection(sets_path[set2])\n",
    "        vals_intersec.append(100* len(inter)/len(sets_rad[set1]))\n",
    "vals_intersec = np.asarray(vals_intersec)\n",
    "vals_intersec = np.reshape(vals_intersec, (k,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,6))\n",
    "plt.imshow(vals_intersec, aspect = 'auto', cmap = 'Blues', vmin = 0, vmax = 100)\n",
    "\n",
    "# Labels\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(k))\n",
    "ax.set_yticks(np.arange(k))\n",
    "ax.set_xticklabels(range(k), fontsize = 18, weight = 'bold')\n",
    "ax.set_yticklabels(range(k), fontsize = 18, weight = 'bold')\n",
    "plt.setp(ax.get_xticklabels(), ha = 'right', rotation_mode = 'anchor')\n",
    "plt.ylabel('Radial', fontsize = 24, weight = 'bold')\n",
    "plt.xlabel('Path', fontsize = 24, weight = 'bold')\n",
    "\n",
    "# Loop over data dimensions and create text annotations to show the percentage of percentage of predictions for each combination\n",
    "for i in range(k):\n",
    "    for j in range(k):\n",
    "        if vals_intersec[i, j] > 50:\n",
    "            text = ax.text(j, i, str(round(vals_intersec[i, j],2)) + '%', ha = 'center', va = 'center', color = 'white', fontsize = 18)\n",
    "        if vals_intersec[i, j] < 50:\n",
    "            text = ax.text(j, i, str(round(vals_intersec[i, j],2)) + '%', ha = 'center', va = 'center', color = 'black', fontsize = 18)\n",
    "\n",
    "plt.title('Intersection in \\nidentified clusters', fontsize = 28, weight = 'bold')         \n",
    "plt.savefig('../Results/Figures/Perce_intersec_clusters_gmms.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Saving in pickle dictionaries the names of the morphos that belong to the different clusters\n",
    "- per metric (radial and path distances)\n",
    "- per clustering method (Kmeans and GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-MEANS\n",
    "names_kmeans_rad = {}\n",
    "for l in np.unique(labels_kmeans_rad):\n",
    "    names_kmeans_rad[l] = sel_names[labels_kmeans_rad == l]\n",
    "save_variable(names_kmeans_rad, '../Results/Saved_variables/Cluster_dict_names_morphos_kmeans_rad')\n",
    "\n",
    "names_kmeans_path = {}\n",
    "for l in np.unique(labels_kmeans_path):\n",
    "    names_kmeans_path[l] = sel_names[labels_kmeans_path == l]\n",
    "save_variable(names_kmeans_path, '../Results/Saved_variables/Cluster_dict_names_morphos_kmeans_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM\n",
    "names_gmm_rad = {}\n",
    "for l in np.unique(labels_gmm_rad):\n",
    "    names_gmm_rad[l] = sel_names[labels_gmm_rad == l]\n",
    "save_variable(names_gmm_rad, '../Results/Saved_variables/Cluster_dict_names_morphos_gmms_rad')\n",
    "\n",
    "names_gmm_path = {}\n",
    "for l in np.unique(labels_gmm_path):\n",
    "    names_gmm_path[l] = sel_names[labels_gmm_path == l]\n",
    "save_variable(names_gmm_path, '../Results/Saved_variables/Cluster_dict_names_morphos_gmms_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### How to reload these variables if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp_dict = load_pickle_variable('../Results/Saved_variables/Cluster_dict_names_morphos_gmms_rad')\n",
    "print(tmp_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2023",
   "language": "python",
   "name": "venv2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
